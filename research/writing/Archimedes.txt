Archimedes

In a tweet: by jointly learning how to maximize the objective function (Bayesian Optimization) as well as the Structural Causal Model (active learning) you can speed up Bayesian optimization in terms of number of samples needed.

[ Archimedes ]

Challenge: Bayesian Optimization is an incredibly useful optimization tool, but can be computationally expensive due to O(N^3) updating. We wish to guide active learning optimally. The causal data generating process of the data contains valuable information that we might use, but no BO model takes this into account.

[ Pipeline ]

Solution: by learning a Structural Causal Model at the same as Bayesian Optimization, we can set a tradeoff between sampling for optimization and sampling for causal understanding.

[ Results Graph ]

Results: I'm able to outperform competing sampling methods. This becomes especially pronounced when there is significant confounding in the data.

this has particular applications to expensive, low-data regimes with causal structures: scientific discovery, control processes, consumer behaviour, policymaking, etc. Long-term, I hope this to be useful in reinforcement learning, so that agents maximize total reward by a combination of following optimal reward-driven policies as well as curiousity policies that learn the DGP.